{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80b2a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Añadir src al path\n",
    "src_path = Path.cwd().parent / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import *\n",
    "\n",
    "BASE_DIR = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "10928bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(f'{BASE_DIR}/data/features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d81e9",
   "metadata": {},
   "source": [
    "### Let's see which model we are gonna use to tackle this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ca98a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Seleccionar features para el modelo\n",
    "# feature_columns = [\n",
    "#     'year_established',\n",
    "#        'annual_revenue', 'total_payroll', \n",
    "#        'num_employees',\n",
    "#        'num_quotes', 'num_products_requested',\n",
    "#        'avg_premium', 'min_premium', 'max_premium', 'carrier_diversity', 'premium_range','num_carriers', 'total_quotes',\n",
    "#        'carrier_concentration', 'product_concentration', 'std_premium','sum_premium',\n",
    "#        'iqr_premium', 'premium_skewness',  'premium_to_revenue_ratio',\n",
    "#        'quotes_per_employee', 'quotes_per_million_revenue'\n",
    "    \n",
    "#     # One-hot encoded (se generan automáticamente)\n",
    "#  ] \n",
    "\n",
    "excluded_features = ['account_uuid', 'state', 'business_structure','industry', 'subindustry',  'index','account_value','region','premium_range_bins','premium_log','premium_log_bins','premium_range']\n",
    "feature_columns = list(set(features.columns) - set(excluded_features))\n",
    "X = features[feature_columns]\n",
    "y = features['account_value']\n",
    "\n",
    "\n",
    "\n",
    "# Preparar datos\n",
    "# Crear bins, por ejemplo 10 percentiles\n",
    "y_binned = pd.qcut(y, q=10, duplicates='drop')  # divide y en 10 grupos\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y_binned)\n",
    "\n",
    "# The reason we are applying this log transformation is to reduce the impact of extreme values (outliers) in the target variable as we have seen in the EDA phase. \n",
    "# This can help improve the model's performance and stability. However, remember to reverse this transformation when interpreting the model's predictions and calculating\n",
    "# metrics on the original scale such as RMSE or MAE.\n",
    "# This is done using the np.expm1 function, which is the inverse of np.log\n",
    "\n",
    "\n",
    "# y_train = np.log1p(y_train)  # log(1 + y)\n",
    "# y_val = np.log1p(y_val)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c1f5ae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost:\n",
      "RMSE: 1290.25\n",
      "MAE: 448.13\n",
      "R²: 0.8704\n",
      "\n",
      "Random Forest:\n",
      "RMSE: 1331.03\n",
      "MAE: 412.34\n",
      "R²: 0.8621\n",
      "\n",
      "LightGBM:\n",
      "RMSE: 1393.39\n",
      "MAE: 450.10\n",
      "R²: 0.8489\n",
      "\n",
      " Best model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Models to compare\n",
    "models = {\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        random_state=42,\n",
    "        reg_lambda= 0.5, \n",
    "        reg_alpha=1.0,\n",
    "        eval_metric='rmse'\n",
    "        \n",
    "    ),\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=7,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf= 1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.01,\n",
    "        random_state=42,\n",
    "        reg_lambda= 0.5, \n",
    "        reg_alpha=1.0,\n",
    "        metric='rmse',\n",
    "        verbosity=-1  \n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    trained_model, rmse, pred = evaluate_model(model, X_train, X_val, y_train, y_val, name)\n",
    "    results[name] = {'model': trained_model, 'rmse': rmse, 'predictions': pred}\n",
    "\n",
    "# Seleccionar mejor modelo\n",
    "best_model_name = min(results.keys(), key=lambda x: results[x]['rmse'])\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"\\n Best model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "79e87e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF - Train R²: 0.9331, Val R²: 0.8621\n",
      "RF - Train R² - Val R²: 0.0710\n",
      "XGB - Train R²: 0.9481, Val R²: 0.8704\n",
      "XGB - Train R² - Val R²: 0.0777\n",
      "LGB - Train R²: 0.7831, Val R²: 0.8489\n",
      "LGB - Train R² - Val R²: -0.0658\n"
     ]
    }
   ],
   "source": [
    "# Now, let's check for overfitting by comparing training and validation R² scores\n",
    "# As per the theory, a significant difference between training and validation R² scores indicates overfitting.\n",
    "# A model that performs well on training data but poorly on validation data is likely overfitting.\n",
    "\n",
    "rf_train_score = results['Random Forest']['model'].score(X_train, y_train)\n",
    "rf_val_score = results['Random Forest']['model'].score(X_val, y_val)\n",
    "print(f\"RF - Train R²: {rf_train_score:.4f}, Val R²: {rf_val_score:.4f}\")\n",
    "print(f\"RF - Train R² - Val R²: {rf_train_score - rf_val_score:.4f}\")\n",
    "\n",
    "xgb_train_score = results['XGBoost']['model'].score(X_train, y_train)\n",
    "xgb_val_score = results['XGBoost']['model'].score(X_val, y_val)\n",
    "print(f\"XGB - Train R²: {xgb_train_score:.4f}, Val R²: {xgb_val_score:.4f}\")\n",
    "print(f\"XGB - Train R² - Val R²: {xgb_train_score - xgb_val_score:.4f}\")\n",
    "\n",
    "lgb_train_score = results['LightGBM']['model'].score(X_train, y_train)\n",
    "lgb_val_score = results['LightGBM']['model'].score(X_val, y_val)\n",
    "print(f\"LGB - Train R²: {lgb_train_score:.4f}, Val R²: {lgb_val_score:.4f}\")\n",
    "print(f\"LGB - Train R² - Val R²: {lgb_train_score - lgb_val_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# We see that the difference between training and validation R² scores is relatively small for all models, \n",
    "# indicating that overfitting is not a significant concern in this case. We are gonna take the best permorming model,\n",
    "#  which is LGBM, and use it for further tuning and final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d7ff368d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      5709.000000\n",
       "mean       1723.704689\n",
       "std        3761.789151\n",
       "min          29.250000\n",
       "25%         500.000000\n",
       "50%         697.000000\n",
       "75%        1438.870000\n",
       "max      134752.410000\n",
       "Name: account_value, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b3582c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 700, 'reg_alpha': 1.0, 'reg_lambda': 0.5, 'subsample': 0.7}\n",
      "Best CV score: 0.733830654507369\n"
     ]
    }
   ],
   "source": [
    "# Parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [500,600, 700],  # Añadir esto\n",
    "    'max_depth': [3,4,5],\n",
    "    'learning_rate': [0.01, 0.02],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [0.6],\n",
    "    'reg_lambda': [0.5],\n",
    "    'reg_alpha': [1.0]\n",
    "}\n",
    "\n",
    "\n",
    "search = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42,eval_metric='rmse',),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the search\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", search.best_params_)\n",
    "print(\"Best CV score:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e4aa04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(**search.best_params_, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f02a0ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Tuned:\n",
      "RMSE: 1272.19\n",
      "MAE: 429.03\n",
      "R²: 0.8740\n"
     ]
    }
   ],
   "source": [
    "trained_model, rmse, pred = evaluate_model(model, X_train, X_val, y_train, y_val, 'XGBoost Tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bab313c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB - Train R²: 0.9700, Val R²: 0.8740\n",
      "XGB - Train R² - Val R²: 0.0960\n"
     ]
    }
   ],
   "source": [
    "xgb_train_score = trained_model.score(X_train, y_train)\n",
    "xgb_val_score = trained_model.score(X_val, y_val)\n",
    "print(f\"XGB - Train R²: {xgb_train_score:.4f}, Val R²: {xgb_val_score:.4f}\")\n",
    "print(f\"XGB - Train R² - Val R²: {xgb_train_score - xgb_val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "916d6528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "# En Jupyter, usar el directorio de trabajo actual\n",
    "BASE_DIR = Path.cwd().parent\n",
    "\n",
    "# Save the model\n",
    "\n",
    "joblib.dump(trained_model,f'{BASE_DIR}/model/xgboost_model.joblib')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 38\n",
      "Feature names: ['log_total_payroll', 'year_established', 'total_payroll', 'product_concentration', 'state_premium_sum_encoded', 'subindustry_sum_premium_encoded', 'carrier_concentration', 'business_structure_revenue_encoded', 'premium_per_employee', 'industry_revenue_encoded', 'num_quotes', 'revenue_x_payroll', 'log_annual_revenue', 'premium_to_revenue_ratio', 'total_quotes', 'premium_ratio_max_avg', 'annual_revenue', 'max_x_nquotes', 'state_revenue_encoded', 'num_employees', 'business_structure_premium_sum_encoded', 'industry_sum_premium_encoded', 'num_products_requested', 'iqr_premium', 'premium_per_revenue', 'premium_range', 'avg_x_nproducts', 'premium_per_quote', 'subindustry_revenue_encoded', 'max_premium', 'quotes_per_million_revenue', 'avg_premium', 'sum_premium', 'carrier_diversity', 'min_premium', 'num_carriers', 'quotes_per_employee', 'revenue_per_employee']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Feature names: {X_train.columns.tolist() if hasattr(X_train, 'columns') else 'No names'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "3848748d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing local model vs API predictions...\n",
      "Local model prediction: 8.119949340820312\n"
     ]
    }
   ],
   "source": [
    "sample_features = [15.2, 3.0, 0.5, 0.8, 25000.0, 1200.0, 0.15, 0.7, 12.0, 0.6, \n",
    "                   8500.0, 500.0, 0.9, 2.5, 0.4, 3000.0, 85000.0, 0.08, 16.5, \n",
    "                   0.12, 25.5, 144.0, 2.8, 102000.0, 0.15, 0.6, 4.0, 0.55, 85.0, \n",
    "                   24500.0, 0.75, 12750000.0]\n",
    "\n",
    "print(\"Comparing local model vs API predictions...\")\n",
    "\n",
    "# Local prediction\n",
    "local_prediction = model.predict(np.array(sample_features).reshape(1, -1))[0]\n",
    "print(f\"Local model prediction: {local_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c811894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-FEFZF9DM-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
